{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting zh-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/zh_core_web_sm-3.8.0/zh_core_web_sm-3.8.0-py3-none-any.whl (48.5 MB)\n",
      "     ---------------------------------------- 0.0/48.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/48.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/48.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/48.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/48.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.3/48.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.3/48.5 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.5/48.5 MB 524.3 kB/s eta 0:01:32\n",
      "     --------------------------------------- 0.5/48.5 MB 524.3 kB/s eta 0:01:32\n",
      "     --------------------------------------- 0.5/48.5 MB 524.3 kB/s eta 0:01:32\n",
      "      -------------------------------------- 0.8/48.5 MB 493.2 kB/s eta 0:01:37\n",
      "      -------------------------------------- 0.8/48.5 MB 493.2 kB/s eta 0:01:37\n",
      "      -------------------------------------- 1.0/48.5 MB 541.2 kB/s eta 0:01:28\n",
      "      -------------------------------------- 1.0/48.5 MB 541.2 kB/s eta 0:01:28\n",
      "     - ------------------------------------- 1.3/48.5 MB 573.6 kB/s eta 0:01:23\n",
      "     - ------------------------------------- 1.6/48.5 MB 630.8 kB/s eta 0:01:15\n",
      "     - ------------------------------------- 1.8/48.5 MB 689.5 kB/s eta 0:01:08\n",
      "     - ------------------------------------- 2.1/48.5 MB 720.5 kB/s eta 0:01:05\n",
      "     -- ------------------------------------ 2.6/48.5 MB 820.6 kB/s eta 0:00:56\n",
      "     -- ------------------------------------ 2.9/48.5 MB 869.2 kB/s eta 0:00:53\n",
      "     -- ------------------------------------ 3.4/48.5 MB 967.9 kB/s eta 0:00:47\n",
      "     -- ------------------------------------ 3.7/48.5 MB 982.4 kB/s eta 0:00:46\n",
      "     --- ------------------------------------ 4.2/48.5 MB 1.1 MB/s eta 0:00:43\n",
      "     --- ------------------------------------ 4.7/48.5 MB 1.1 MB/s eta 0:00:39\n",
      "     ---- ----------------------------------- 5.2/48.5 MB 1.2 MB/s eta 0:00:36\n",
      "     ---- ----------------------------------- 5.8/48.5 MB 1.3 MB/s eta 0:00:35\n",
      "     ----- ---------------------------------- 6.3/48.5 MB 1.3 MB/s eta 0:00:32\n",
      "     ----- ---------------------------------- 7.1/48.5 MB 1.4 MB/s eta 0:00:30\n",
      "     ------ --------------------------------- 7.3/48.5 MB 1.4 MB/s eta 0:00:29\n",
      "     ------ --------------------------------- 7.9/48.5 MB 1.5 MB/s eta 0:00:28\n",
      "     ------- -------------------------------- 8.9/48.5 MB 1.6 MB/s eta 0:00:25\n",
      "     ------- -------------------------------- 9.7/48.5 MB 1.7 MB/s eta 0:00:24\n",
      "     -------- ------------------------------- 10.2/48.5 MB 1.7 MB/s eta 0:00:23\n",
      "     -------- ------------------------------- 10.7/48.5 MB 1.8 MB/s eta 0:00:22\n",
      "     --------- ------------------------------ 11.0/48.5 MB 1.8 MB/s eta 0:00:22\n",
      "     --------- ------------------------------ 11.5/48.5 MB 1.7 MB/s eta 0:00:22\n",
      "     ---------- ----------------------------- 12.8/48.5 MB 1.9 MB/s eta 0:00:20\n",
      "     ----------- ---------------------------- 13.6/48.5 MB 1.9 MB/s eta 0:00:19\n",
      "     ------------ --------------------------- 15.5/48.5 MB 2.1 MB/s eta 0:00:16\n",
      "     ------------- -------------------------- 16.8/48.5 MB 2.2 MB/s eta 0:00:15\n",
      "     --------------- ------------------------ 18.4/48.5 MB 2.4 MB/s eta 0:00:13\n",
      "     ---------------- ----------------------- 19.7/48.5 MB 2.5 MB/s eta 0:00:12\n",
      "     ---------------- ----------------------- 20.2/48.5 MB 2.5 MB/s eta 0:00:12\n",
      "     ----------------- ---------------------- 20.7/48.5 MB 2.5 MB/s eta 0:00:12\n",
      "     ----------------- ---------------------- 21.0/48.5 MB 2.5 MB/s eta 0:00:12\n",
      "     ------------------ --------------------- 22.5/48.5 MB 2.6 MB/s eta 0:00:11\n",
      "     ------------------- -------------------- 23.3/48.5 MB 2.6 MB/s eta 0:00:10\n",
      "     ------------------- -------------------- 23.9/48.5 MB 2.6 MB/s eta 0:00:10\n",
      "     -------------------- ------------------- 24.9/48.5 MB 2.7 MB/s eta 0:00:09\n",
      "     ---------------------- ----------------- 26.7/48.5 MB 2.8 MB/s eta 0:00:08\n",
      "     ---------------------- ----------------- 27.8/48.5 MB 2.9 MB/s eta 0:00:08\n",
      "     ------------------------ --------------- 29.9/48.5 MB 3.0 MB/s eta 0:00:07\n",
      "     -------------------------- ------------- 31.7/48.5 MB 3.1 MB/s eta 0:00:06\n",
      "     --------------------------- ------------ 32.8/48.5 MB 3.2 MB/s eta 0:00:05\n",
      "     ----------------------------- ---------- 35.9/48.5 MB 3.4 MB/s eta 0:00:04\n",
      "     ------------------------------- -------- 38.0/48.5 MB 3.5 MB/s eta 0:00:03\n",
      "     -------------------------------- ------- 39.8/48.5 MB 3.6 MB/s eta 0:00:03\n",
      "     ---------------------------------- ----- 41.9/48.5 MB 3.8 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 43.0/48.5 MB 3.8 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 44.0/48.5 MB 3.8 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 46.9/48.5 MB 4.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  48.5/48.5 MB 4.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 48.5/48.5 MB 4.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy-pkuseg<2.0.0,>=1.0.0 in c:\\users\\johnny\\miniconda3\\envs\\jenv\\lib\\site-packages (from zh-core-web-sm==3.8.0) (1.0.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.3.0 in c:\\users\\johnny\\miniconda3\\envs\\jenv\\lib\\site-packages (from spacy-pkuseg<2.0.0,>=1.0.0->zh-core-web-sm==3.8.0) (2.5.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=2.0.0 in c:\\users\\johnny\\miniconda3\\envs\\jenv\\lib\\site-packages (from spacy-pkuseg<2.0.0,>=1.0.0->zh-core-web-sm==3.8.0) (2.2.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in c:\\users\\johnny\\miniconda3\\envs\\jenv\\lib\\site-packages (from srsly<3.0.0,>=2.3.0->spacy-pkuseg<2.0.0,>=1.0.0->zh-core-web-sm==3.8.0) (2.0.10)\n",
      "Installing collected packages: zh-core-web-sm\n",
      "Successfully installed zh-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('zh_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download zh_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>詞</th>\n",
       "      <th>詞類</th>\n",
       "      <th>詞性標注</th>\n",
       "      <th>單詞依存關係</th>\n",
       "      <th>是否為純字母組成</th>\n",
       "      <th>是否為停用詞</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>台灣</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>是</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VC</td>\n",
       "      <td>cop</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>一</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>nummod</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>個</td>\n",
       "      <td>NUM</td>\n",
       "      <td>M</td>\n",
       "      <td>mark:clf</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>位於</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>亞洲</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VV</td>\n",
       "      <td>compound:vc</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>東部</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VV</td>\n",
       "      <td>acl</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>的</td>\n",
       "      <td>PART</td>\n",
       "      <td>DEC</td>\n",
       "      <td>mark</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>島嶼</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>compound:nn</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>國家</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>。</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>PU</td>\n",
       "      <td>punct</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     詞     詞類 詞性標注       單詞依存關係  是否為純字母組成  是否為停用詞\n",
       "0   台灣   NOUN   NN        nsubj      True   False\n",
       "1    是   VERB   VC          cop      True    True\n",
       "2    一    NUM   CD       nummod      True    True\n",
       "3    個    NUM    M     mark:clf      True   False\n",
       "4   位於   NOUN   NN        nsubj      True   False\n",
       "5   亞洲   VERB   VV  compound:vc      True   False\n",
       "6   東部   VERB   VV          acl      True   False\n",
       "7    的   PART  DEC         mark      True    True\n",
       "8   島嶼   NOUN   NN  compound:nn      True   False\n",
       "9   國家   NOUN   NN         ROOT      True   False\n",
       "10   。  PUNCT   PU        punct     False    True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "nlp_zh = spacy.load(\"zh_core_web_sm\")\n",
    "\n",
    "# 對中文語句進行解析\n",
    "doc = nlp_zh('台灣是一個位於亞洲東部的島嶼國家。')\n",
    "\n",
    "columns=['詞', '詞類', '詞性標注', '單詞依存關係', '是否為純字母組成', '是否為停用詞']\n",
    "dim = list(map(lambda x: [x.text, x.pos_, x.tag_, x.dep_, x.is_alpha, x.is_stop], doc))\n",
    "pd.DataFrame(dim, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>詞</th>\n",
       "      <th>詞類</th>\n",
       "      <th>詞性標注</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>台</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>灣</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>亞</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>洲</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   詞  詞類  詞性標注\n",
       "0  台   0     1\n",
       "1  灣   1     2\n",
       "2  亞   7     8\n",
       "3  洲   8     9"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "\n",
    "# 使用適合中文的 Hugging Face 模型\n",
    "model_name = \"ckiplab/bert-base-chinese-ner\"  # 這是台灣 CKIP 提供的 BERT NER 模型\n",
    "\n",
    "# 加載模型與 tokenizer\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 建立 NER pipeline\n",
    "ner = pipeline('ner', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# 測試 NER 功能\n",
    "text = \"台灣是一個位於亞洲東部的島嶼國家。\"\n",
    "doc = ner(text)\n",
    "columns=['詞', '詞類', '詞性標注']\n",
    "dim = list(map(lambda x: [x['word'], x['start'], x['end']], doc))\n",
    "pd.DataFrame(dim, columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-address', 'score': np.float32(0.8082326), 'index': 1, 'word': '台', 'start': 0, 'end': 1}, {'entity': 'I-address', 'score': np.float32(0.5574732), 'index': 2, 'word': '灣', 'start': 1, 'end': 2}, {'entity': 'I-address', 'score': np.float32(0.6146184), 'index': 9, 'word': '洲', 'start': 8, 'end': 9}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>詞</th>\n",
       "      <th>詞類</th>\n",
       "      <th>詞性標注</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>台</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>灣</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>洲</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   詞  詞類  詞性標注\n",
       "0  台   0     1\n",
       "1  灣   1     2\n",
       "2  洲   8     9"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, AutoConfig\n",
    " \n",
    "model = AutoModelForTokenClassification.from_pretrained('uer/roberta-base-finetuned-cluener2020-chinese')\n",
    "tokenizer = AutoTokenizer.from_pretrained('uer/roberta-base-finetuned-cluener2020-chinese')\n",
    "\n",
    "ner = pipeline('ner', model=model, tokenizer=tokenizer)\n",
    "doc = ner('台灣是一個位於亞洲東部的島嶼國家。')\n",
    "print(doc)\n",
    "columns=['詞', '詞類', '詞性標注']\n",
    "dim = list(map(lambda x: [x['word'], x['start'], x['end']], doc))\n",
    "pd.DataFrame(dim, columns=columns)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

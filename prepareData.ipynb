{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用 CPU 運算\n",
      "已處理 274 個詞...\n",
      "已處理 314 個詞...\n",
      "已處理 317 個詞...\n",
      "已處理 322 個詞...\n",
      "已處理 328 個詞...\n",
      "已處理 326 個詞...\n",
      "已處理 317 個詞...\n",
      "已處理 315 個詞...\n",
      "已處理 297 個詞...\n",
      "已處理 321 個詞...\n",
      "已處理 326 個詞...\n",
      "已處理 322 個詞...\n",
      "已處理 317 個詞...\n",
      "已處理 318 個詞...\n",
      "已處理 328 個詞...\n",
      "已處理 325 個詞...\n",
      "已處理 313 個詞...\n",
      "已處理 321 個詞...\n",
      "已處理 315 個詞...\n",
      "已處理 308 個詞...\n",
      "已處理 333 個詞...\n",
      "已處理 308 個詞...\n",
      "已處理 322 個詞...\n",
      "已處理 328 個詞...\n",
      "已處理 303 個詞...\n",
      "已處理 316 個詞...\n",
      "已處理 317 個詞...\n",
      "已處理 319 個詞...\n",
      "已處理 318 個詞...\n",
      "已處理 311 個詞...\n",
      "已處理 310 個詞...\n",
      "已處理 318 個詞...\n",
      "已處理 318 個詞...\n",
      "已處理 305 個詞...\n",
      "已處理 309 個詞...\n",
      "已處理 312 個詞...\n",
      "已處理 326 個詞...\n",
      "已處理 328 個詞...\n",
      "已處理 308 個詞...\n",
      "已處理 320 個詞...\n",
      "已處理 311 個詞...\n",
      "已處理 316 個詞...\n",
      "已處理 326 個詞...\n",
      "已處理 318 個詞...\n",
      "已處理 303 個詞...\n",
      "已處理 312 個詞...\n",
      "已處理 308 個詞...\n",
      "已處理 308 個詞...\n",
      "已處理 315 個詞...\n",
      "已處理 313 個詞...\n",
      "已處理 294 個詞...\n",
      "已處理 323 個詞...\n",
      "已處理 312 個詞...\n",
      "已處理 320 個詞...\n",
      "已處理 321 個詞...\n",
      "已處理 316 個詞...\n",
      "已處理 323 個詞...\n",
      "已處理 319 個詞...\n",
      "已處理 18 個詞...\n",
      "\n",
      "第一章 - O\n",
      "　 - O\n",
      "滅門 - O\n",
      "\n",
      " - O\n",
      "\n",
      "\n",
      "________________________________________ - O\n",
      "\n",
      " - O\n",
      "\n",
      "\n",
      "　 - O\n",
      "　 - O\n",
      "和 - O\n",
      "風熏 - O\n",
      "柳 - O\n",
      "， - O\n",
      "花香 - O\n",
      "醉人 - O\n",
      "， - O\n",
      "正是 - O\n",
      "南國 - O\n",
      "春光 - O\n",
      "漫爛 - O\n",
      "季節 - O\n",
      "。 - O\n",
      "\n",
      "\n",
      "福建省 - O\n",
      "福州 - O\n",
      "府 - O\n",
      "西門 - O\n",
      "大街 - O\n",
      "， - O\n",
      "青石板路 - O\n",
      "筆直 - O\n",
      "的 - O\n",
      "伸展出去 - O\n",
      "， - O\n",
      "直通 - O\n",
      "西門 - O\n",
      "。 - O\n",
      "\n",
      "\n",
      "一座 - O\n",
      "建構 - O\n",
      "宏偉 - O\n",
      "的 - O\n",
      "宅第 - O\n",
      "之前 - O\n",
      "， - O\n",
      "左右 - O\n",
      "兩座 - O\n",
      "石壇 - O\n",
      "中 - O\n",
      "各 - O\n",
      "豎 - O\n",
      "一根 - O\n",
      "兩丈 - O\n",
      "來 - O\n",
      "高 - O\n",
      "的 - O\n",
      "旗杆 - O\n",
      "， - O\n",
      "杆頂 - O\n",
      "飄揚 - O\n",
      "青旗 - O\n",
      "。 - O\n",
      "\n",
      "\n",
      "右首 - O\n",
      "旗 - O\n",
      "上黃色 - O\n",
      "絲線 - O\n",
      "繡 - O\n",
      "著 - O\n",
      "一頭 - O\n",
      "張牙舞 - O\n",
      "爪 - O\n",
      "、 - O\n",
      "神態 - O\n",
      "威猛 - O\n",
      "的 - O\n",
      "雄獅 - O\n",
      "， - O\n",
      "旗子 - O\n",
      "隨風 - O\n",
      "招展 - O\n",
      "， - O\n",
      "顯得 - O\n",
      "雄獅 - O\n",
      "更 - O\n",
      "奕奕 - O\n",
      "若生 - O\n",
      "。 - O\n",
      "\n",
      "\n",
      "雄獅頭 - O\n",
      "頂 - O\n",
      "有 - O\n",
      "一 - O\n",
      "對 - O\n",
      "黑絲線 - O\n",
      "繡 - O\n",
      "的 - O\n",
      "蝙蝠 - O\n",
      "展翅 - O\n",
      "飛翔 - O\n",
      "。 - O\n",
      "\n",
      "\n",
      "左首 - O\n",
      "旗上 - O\n",
      "繡 - O\n",
      "著 - O\n",
      "“ - O\n",
      "福威 - O\n",
      "鏢局 - O\n",
      "” - O\n",
      "四個 - O\n",
      "黑字 - O\n",
      "， - O\n",
      "銀鉤 - O\n",
      "鐵劃 - O\n",
      "， - O\n",
      "剛勁 - O\n",
      "非凡 - O\n",
      "。 - O\n",
      "\n",
      "\n",
      "大宅 - O\n",
      "朱漆 - O\n",
      "大門 - O\n",
      "， - O\n",
      "門上 - O\n",
      "茶杯 - O\n",
      "大小 - O\n",
      "的 - O\n",
      "銅釘 - O\n",
      "閃閃 - O\n",
      "發光 - O\n",
      "， - O\n",
      "門頂 - O\n",
      "匾額 - O\n",
      "寫 - O\n",
      "著 - O\n",
      "“ - O\n",
      "福威 - O\n",
      "鏢局 - O\n",
      "” - O\n",
      "四個 - O\n",
      "金漆 - O\n",
      "大字 - O\n",
      "， - O\n",
      "下 - O\n",
      "麵 - O\n",
      "橫書 - O\n",
      "“ - O\n",
      "總號 - O\n",
      "”...\n",
      "\n",
      "\n",
      "處理完成！共識別了 18329 個詞，分成 1068 個句子。\n",
      "結果已保存至 output_ner_results_formatted.txt\n",
      "注意：已按要求僅保留 ORG, PER, LOC 標籤，其他標籤均轉為 O\n",
      "\n",
      "測試樣例：\n",
      "已處理 15 個詞...\n",
      "\n",
      "企業 - O\n",
      "在 - O\n",
      "AI - O\n",
      "應用 - O\n",
      "落地 - O\n",
      "策略 - O\n",
      "的 - O\n",
      "推展 - O\n",
      "不應 - O\n",
      "僅視 - O\n",
      "為 - O\n",
      "單獨 - O\n",
      "專案 - O\n",
      "執行 - O\n",
      "。 - O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import jieba\n",
    "import numpy as np\n",
    "\n",
    "# 加載模型和分詞器\n",
    "model_name = \"ckiplab/bert-base-chinese-ner\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# 設置設備（使用 GPU 若可用，包括 CUDA 和 Apple MPS）\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"使用 CUDA GPU 加速\")\n",
    "elif hasattr(torch, 'backends') and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"使用 Apple Metal (MPS) 加速\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"使用 CPU 運算\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()  # 設置模型為評估模式\n",
    "\n",
    "# 讀取文本文件\n",
    "with open('./data/moneynews20250313.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# 分詞並記錄位置\n",
    "words = list(jieba.cut(text))\n",
    "word_list = []\n",
    "start = 0\n",
    "for word in words:\n",
    "    word_start = text.find(word, start)\n",
    "    word_end = word_start + len(word)\n",
    "    word_list.append((word, word_start, word_end))\n",
    "    start = word_end\n",
    "\n",
    "# 定義獲取單詞標籤的函數\n",
    "def get_entity_type(word_start, word_end, offset_mapping, predicted_labels, label_names):\n",
    "    \"\"\"\n",
    "    獲取單詞的實體類型\n",
    "    \"\"\"\n",
    "    word_token_indices = []\n",
    "    \n",
    "    # 找出覆蓋這個詞的所有標記\n",
    "    for i, (token_start, token_end) in enumerate(offset_mapping):\n",
    "        if token_end > word_start and token_start < word_end:\n",
    "            word_token_indices.append(i)\n",
    "    \n",
    "    if not word_token_indices:\n",
    "        return 'O'\n",
    "    \n",
    "    # 獲取每個標記的標籤\n",
    "    token_labels = [label_names[predicted_labels[i]] for i in word_token_indices]\n",
    "    \n",
    "    # 檢查是否有B-開頭的標籤\n",
    "    b_labels = [label for label in token_labels if label.startswith('B-')]\n",
    "    if b_labels:\n",
    "        # 如果有B-開頭的標籤，返回其實體類型\n",
    "        entity_type = b_labels[0].split('-')[1]\n",
    "        return entity_type\n",
    "    \n",
    "    # 檢查是否有I-開頭的標籤\n",
    "    i_labels = [label for label in token_labels if label.startswith('I-')]\n",
    "    if i_labels:\n",
    "        # 如果有I-開頭的標籤，返回其實體類型\n",
    "        entity_type = i_labels[0].split('-')[1]\n",
    "        return entity_type\n",
    "    \n",
    "    # 如果沒有B-或I-開頭的標籤，返回O\n",
    "    return 'O'\n",
    "\n",
    "# 分塊處理長文本\n",
    "def process_text_in_chunks(text, word_list, max_length=480):  # 設置小於512的最大長度，留出一些餘量\n",
    "    results = []  # 存儲每個詞的標籤\n",
    "    \n",
    "    # 按字符位置將文本分成多個塊\n",
    "    chunks = []\n",
    "    current_chunk_start = 0\n",
    "    current_chunk_words = []\n",
    "    \n",
    "    for word, start, end in word_list:\n",
    "        # 如果這個詞會使當前塊超過最大長度，則開始新塊\n",
    "        if end - current_chunk_start > max_length and current_chunk_words:\n",
    "            chunks.append((current_chunk_start, current_chunk_words))\n",
    "            current_chunk_start = start\n",
    "            current_chunk_words = []\n",
    "        \n",
    "        # 將詞添加到當前塊\n",
    "        current_chunk_words.append((word, start, end))\n",
    "    \n",
    "    # 添加最後一個塊\n",
    "    if current_chunk_words:\n",
    "        chunks.append((current_chunk_start, current_chunk_words))\n",
    "    \n",
    "    # 處理每個塊\n",
    "    for chunk_start, chunk_words in chunks:\n",
    "        # 提取塊的文本\n",
    "        if len(chunk_words) == 0:\n",
    "            continue\n",
    "            \n",
    "        chunk_end = chunk_words[-1][2]\n",
    "        chunk_text = text[chunk_start:chunk_end]\n",
    "        \n",
    "        # 標記化文本\n",
    "        encoding = tokenizer(chunk_text, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "        offset_mapping = encoding.pop(\"offset_mapping\")[0]  # 取出 offset_mapping 並保存\n",
    "        \n",
    "        # 將輸入移至設備\n",
    "        input_ids = encoding[\"input_ids\"].to(device)\n",
    "        attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "        \n",
    "        # 進行推理\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # 獲取預測標籤\n",
    "        predicted_labels = outputs.logits[0].argmax(dim=-1).cpu().numpy()\n",
    "        label_names = [model.config.id2label[i] for i in range(len(model.config.id2label))]\n",
    "        \n",
    "        # 獲取每個單詞的標籤\n",
    "        for word, start, end in chunk_words:\n",
    "            # 需要調整在塊內的相對位置\n",
    "            relative_start = start - chunk_start\n",
    "            relative_end = end - chunk_start\n",
    "            label = get_entity_type(relative_start, relative_end, offset_mapping, predicted_labels, label_names)\n",
    "            results.append((word, label))\n",
    "        \n",
    "        print(f\"已處理 {len(chunk_words)} 個詞...\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 處理文本並獲取結果\n",
    "all_pairs = process_text_in_chunks(text, word_list)\n",
    "\n",
    "# 定義處理句子的函數\n",
    "def format_sentence_output(sentence, pairs):\n",
    "    \"\"\"\n",
    "    將一個句子的標籤結果按指定格式輸出，僅列出 ORG, PER, LOC 標籤\n",
    "    \"\"\"\n",
    "    # result = f\"來源：{sentence}\\n輸出：\\n\"\n",
    "    result = f\"\\n\"\n",
    "    for word, label in pairs:\n",
    "        # 僅顯示 ORG, PER, LOC 標籤，其他都標為 O\n",
    "        if label in ['ORG', 'PER', 'LOC']:\n",
    "            result += f\"{word} - {label}\\n\"\n",
    "        else:\n",
    "            result += f\"{word} - O\\n\"\n",
    "    return result\n",
    "\n",
    "# 將結果按句子分組\n",
    "sentences = []\n",
    "current_sentence = \"\"\n",
    "current_pairs = []\n",
    "\n",
    "for word, label in all_pairs:\n",
    "    current_sentence += word\n",
    "    current_pairs.append((word, label))\n",
    "    \n",
    "    # 使用標點符號來判斷句子結束\n",
    "    if word in [\"。\", \"！\", \"？\", \"…\", \"\\n\"]:\n",
    "        if current_sentence.strip():  # 確保句子不為空\n",
    "            sentences.append((current_sentence, current_pairs))\n",
    "        current_sentence = \"\"\n",
    "        current_pairs = []\n",
    "\n",
    "# 處理最後一個句子（如果有）\n",
    "if current_sentence.strip():\n",
    "    sentences.append((current_sentence, current_pairs))\n",
    "\n",
    "# 輸出結果\n",
    "output = \"\"\n",
    "for sentence, pairs in sentences:\n",
    "    formatted_output = format_sentence_output(sentence, pairs)\n",
    "    # 移除多餘的換行，使格式符合要求\n",
    "    formatted_output = formatted_output.replace(\"\\n輸出：\\n\", \"\")\n",
    "    output += formatted_output + \"\\n\"\n",
    "    \n",
    "# 打印前幾個句子作為示例\n",
    "print(output[:1000] + \"...\\n\")\n",
    "\n",
    "# 將結果寫入文件\n",
    "with open('./output_ner_results_formatted.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(output)\n",
    "\n",
    "print(f\"\\n處理完成！共識別了 {len(all_pairs)} 個詞，分成 {len(sentences)} 個句子。\")\n",
    "print(f\"結果已保存至 output_ner_results_formatted.txt\")\n",
    "print(\"注意：已按要求僅保留 ORG, PER, LOC 標籤，其他標籤均轉為 O\")\n",
    "\n",
    "# 測試樣例\n",
    "test_text = \"企業在AI應用落地策略的推展不應僅視為單獨專案執行。\"\n",
    "print(\"\\n測試樣例：\")\n",
    "test_words = list(jieba.cut(test_text))\n",
    "test_word_list = []\n",
    "start = 0\n",
    "for word in test_words:\n",
    "    word_start = test_text.find(word, start)\n",
    "    word_end = word_start + len(word)\n",
    "    test_word_list.append((word, word_start, word_end))\n",
    "    start = word_end\n",
    "\n",
    "test_results = process_text_in_chunks(test_text, test_word_list)\n",
    "print(format_sentence_output(test_text, test_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

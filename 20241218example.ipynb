{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下載Hugging face shibing624/bert4ner-base-chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 17:04:23,991 - INFO - 正在載入模型: shibing624/bert4ner-base-chinese\n",
      "2024-12-18 17:04:24,253 - INFO - 模型載入完成\n",
      "2024-12-18 17:04:24,253 - INFO - 處理文本: 中華民國民眾黨主席柯文哲涉政治獻金假帳案，調查局北機站清查金流發現，民眾黨利用「網紅帶貨」銷售手法，先藉由「學姐」黃瀞瑩等人高知名度，吸引選民捐贈政治獻金，再用「折扣碼」發放KP競選小物，進而從中抽佣分潤，抽佣的錢疑來自政治獻金，涉及違反政治獻金法，黃瀞瑩與「戰狼小姐姐」陳智菡、許甫、吳怡萱等4人恐由證人轉列被告偵辦\n",
      "2024-12-18 17:04:24,288 - INFO - 識別結果: [['中 華 民 國 民 眾 黨', 'ORG'], ['柯 文 哲', 'PER'], ['黃 瀞 瑩', 'PER'], ['黃 瀞 瑩', 'PER'], ['陳 智 菡', 'PER'], ['許 甫', 'PER'], ['吳 怡 萱', 'PER']]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# 設置日誌\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def setup_model(model_name: str = \"shibing624/bert4ner-base-chinese\"):\n",
    "    \"\"\"\n",
    "    設置和初始化模型\n",
    "    \n",
    "    Args:\n",
    "        model_name: Hugging Face 模型名稱\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"正在載入模型: {model_name}\") \n",
    "        classifier = pipeline(\n",
    "            \"token-classification\",\n",
    "            model=model_name,\n",
    "            aggregation_strategy=\"simple\"\n",
    "        )\n",
    "        logger.info(\"模型載入完成\")\n",
    "        return classifier\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"模型載入失敗: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def process_text(text: str, classifier) -> list:\n",
    "    \"\"\"\n",
    "    處理文本並進行命名實體識別\n",
    "    \n",
    "    Args:\n",
    "        text: 輸入文本\n",
    "        classifier: NER pipeline\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"處理文本: {text}\")\n",
    "        results = classifier(text)\n",
    "        \n",
    "        # 整理結果\n",
    "        entities = []\n",
    "        for item in results:\n",
    "            entities.append([\n",
    "                item['word'],\n",
    "                item['entity_group']\n",
    "            ])\n",
    "        \n",
    "        return entities\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"文本處理失敗: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # 初始化模型\n",
    "        classifier = setup_model()\n",
    "        \n",
    "        # 測試文本\n",
    "        sample_text = \"中華民國民眾黨主席柯文哲涉政治獻金假帳案，調查局北機站清查金流發現，民眾黨利用「網紅帶貨」銷售手法，先藉由「學姐」黃瀞瑩等人高知名度，吸引選民捐贈政治獻金，再用「折扣碼」發放KP競選小物，進而從中抽佣分潤，抽佣的錢疑來自政治獻金，涉及違反政治獻金法，黃瀞瑩與「戰狼小姐姐」陳智菡、許甫、吳怡萱等4人恐由證人轉列被告偵辦\"\n",
    "        \n",
    "        # 處理文本\n",
    "        entities = process_text(sample_text, classifier)\n",
    "        \n",
    "        logger.info(f\"識別結果: {entities}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"程序執行出錯: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 引用本地端\n",
    "\n",
    "設置模型路徑\n",
    "\n",
    "model_path = \"./examples/outputs/cner_bertsoftmax/best_model\"  # 請確保這個路徑指向你的模型目錄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 17:03:55,110 - INFO - 正在從本地載入模型: ./examples/outputs/cner_bertspan/best_model\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ./examples/outputs/cner_bertspan/best_model and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-12-18 17:03:55,158 - INFO - 模型載入完成\n",
      "2024-12-18 17:03:55,159 - INFO - 處理文本: 您好，我是常建良有多模態客服機器人開發應用，(北京國科會、玉山、電商客服開發經驗)。支援語音、文字、檔案上傳\n",
      "2024-12-18 17:03:55,175 - INFO - 識別結果: []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "from torch import nn\n",
    "import json\n",
    "\n",
    "# 設置日誌\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_model_config(model_path: str):\n",
    "    \"\"\"\n",
    "    載入模型配置\n",
    "    \"\"\"\n",
    "    try:\n",
    "        config_path = os.path.join(model_path, \"config.json\")\n",
    "        with open(config_path, 'r', encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "        return config\n",
    "    except Exception as e:\n",
    "        logger.error(f\"載入配置文件失敗: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def setup_model(model_path: str = \"./best_model\"):\n",
    "    \"\"\"\n",
    "    從本地檔案載入模型\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"正在從本地載入模型: {model_path}\")\n",
    "        \n",
    "        # 檢查必要文件\n",
    "        required_files = [\n",
    "            \"config.json\",\n",
    "            \"model.safetensors\",  # 使用 safetensors 而不是 pytorch_model.bin\n",
    "            \"tokenizer_config.json\",\n",
    "            \"vocab.txt\"\n",
    "        ]\n",
    "        \n",
    "        for file in required_files:\n",
    "            file_path = os.path.join(model_path, file)\n",
    "            if not os.path.exists(file_path):\n",
    "                logger.warning(f\"注意: 找不到文件: {file}\")\n",
    "        \n",
    "        # 載入 tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_path,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        \n",
    "        # 載入模型\n",
    "        model = AutoModelForTokenClassification.from_pretrained(\n",
    "            model_path,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        \n",
    "        # 設置為評估模式\n",
    "        model.eval()\n",
    "        \n",
    "        logger.info(\"模型載入完成\")\n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"模型載入失敗: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def process_text(text: str, model, tokenizer) -> list:\n",
    "    \"\"\"\n",
    "    處理文本並進行命名實體識別\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"處理文本: {text}\")\n",
    "        \n",
    "        # 讀取標籤映射\n",
    "        config = load_model_config(model.config.name_or_path)\n",
    "        id2label = config.get('id2label', {})\n",
    "        \n",
    "        # 對文本進行編碼\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        # 進行預測\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.argmax(outputs.logits, dim=2)\n",
    "        \n",
    "        # 解碼結果\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "        predictions = predictions[0].tolist()\n",
    "        \n",
    "        # 整理實體\n",
    "        entities = []\n",
    "        current_entity = []\n",
    "        current_label = None\n",
    "        \n",
    "        for token, pred_id in zip(tokens, predictions):\n",
    "            if token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "                continue\n",
    "                \n",
    "            label = id2label.get(str(pred_id), 'O')\n",
    "            \n",
    "            if label.startswith('B-'):\n",
    "                if current_entity:\n",
    "                    entities.append([''.join(current_entity), current_label])\n",
    "                current_entity = [token.replace('##', '')]\n",
    "                current_label = label[2:]\n",
    "            elif label.startswith('I-') and current_entity:\n",
    "                current_entity.append(token.replace('##', ''))\n",
    "            elif label == 'O':\n",
    "                if current_entity:\n",
    "                    entities.append([''.join(current_entity), current_label])\n",
    "                    current_entity = []\n",
    "                    current_label = None\n",
    "        \n",
    "        if current_entity:\n",
    "            entities.append([''.join(current_entity), current_label])\n",
    "        \n",
    "        return entities\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"文本處理失敗: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # 設置模型路徑\n",
    "        model_path = \"./examples/outputs/cner_bertspan/best_model\"\n",
    "        \n",
    "        # 檢查模型目錄是否存在\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"找不到模型目錄: {model_path}\")\n",
    "        \n",
    "        # 初始化模型\n",
    "        model, tokenizer = setup_model(model_path)\n",
    "        \n",
    "        # 測試文本\n",
    "        sample_text = \"您好，我是常建良有多模態客服機器人開發應用，(北京國科會、玉山、電商客服開發經驗)。支援語音、文字、檔案上傳\"\n",
    "        \n",
    "        # 處理文本\n",
    "        entities = process_text(sample_text, model, tokenizer)\n",
    "        \n",
    "        logger.info(f\"識別結果: {entities}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"程序執行出錯: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "預測結果：\n",
      "\n",
      "文本: 李明在上海的騰訊公司擔任工程師\n",
      "識別實體:\n",
      "- 明在 (PER)\n",
      "- 海的 (LOC)\n",
      "- 訊公司擔 (ORG)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "# 設置日誌\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class NERModel:\n",
    "    def __init__(self, model_name: str):\n",
    "        \"\"\"\n",
    "        初始化 NER 模型\n",
    "        \n",
    "        Args:\n",
    "            model_name: 模型名稱或路徑\n",
    "        \"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # 載入標籤映射\n",
    "        self.id2label = self.model.config.id2label\n",
    "        self.label2id = self.model.config.label2id\n",
    "\n",
    "    def predict(self, sentences, split_on_space=False):\n",
    "        \"\"\"\n",
    "        進行命名實體識別預測\n",
    "        \n",
    "        Args:\n",
    "            sentences: 輸入句子列表\n",
    "            split_on_space: 是否按空格分割（中文設為 False）\n",
    "        \n",
    "        Returns:\n",
    "            predictions: 預測標籤\n",
    "            raw_outputs: 原始輸出\n",
    "            entities: 識別出的實體\n",
    "        \"\"\"\n",
    "        if isinstance(sentences, str):\n",
    "            sentences = [sentences]\n",
    "            \n",
    "        # 對文本進行編碼\n",
    "        inputs = self.tokenizer(\n",
    "            sentences,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # 進行預測\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            predictions = torch.argmax(outputs.logits, dim=2)\n",
    "        \n",
    "        # 處理結果\n",
    "        batch_entities = []\n",
    "        for sent_idx, (sent, preds) in enumerate(zip(sentences, predictions)):\n",
    "            sent_tokens = self.tokenizer.tokenize(sent)\n",
    "            sent_token_ids = self.tokenizer.convert_tokens_to_ids(sent_tokens)\n",
    "            \n",
    "            entities = []\n",
    "            current_entity = []\n",
    "            current_label = None\n",
    "            \n",
    "            for token, pred_id in zip(sent_tokens, preds):\n",
    "                if token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "                    continue\n",
    "                    \n",
    "                label = self.id2label[pred_id.item()]\n",
    "                \n",
    "                if label.startswith('B-'):\n",
    "                    if current_entity:\n",
    "                        entities.append({\n",
    "                            'entity': ''.join(current_entity).replace('##', ''),\n",
    "                            'label': current_label,\n",
    "                            'type': current_label\n",
    "                        })\n",
    "                    current_entity = [token]\n",
    "                    current_label = label[2:]\n",
    "                elif label.startswith('I-') and current_label == label[2:]:\n",
    "                    current_entity.append(token)\n",
    "                elif label == 'O':\n",
    "                    if current_entity:\n",
    "                        entities.append({\n",
    "                            'entity': ''.join(current_entity).replace('##', ''),\n",
    "                            'label': current_label,\n",
    "                            'type': current_label\n",
    "                        })\n",
    "                        current_entity = []\n",
    "                        current_label = None\n",
    "            \n",
    "            if current_entity:\n",
    "                entities.append({\n",
    "                    'entity': ''.join(current_entity).replace('##', ''),\n",
    "                    'label': current_label,\n",
    "                    'type': current_label\n",
    "                })\n",
    "            \n",
    "            batch_entities.append(entities)\n",
    "        \n",
    "        return predictions.tolist(), outputs, batch_entities\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # 初始化模型\n",
    "        model = NERModel(\"shibing624/bertspan4ner-base-chinese\")\n",
    "        \n",
    "        # 測試文本\n",
    "        sentences = [\n",
    "            \"李明在上海的騰訊公司擔任工程師\"\n",
    "        ]\n",
    "        \n",
    "        # 進行預測\n",
    "        predictions, raw_outputs, entities = model.predict(sentences, split_on_space=False)\n",
    "        \n",
    "        # 輸出結果\n",
    "        print(\"預測結果：\")\n",
    "        for sent, ents in zip(sentences, entities):\n",
    "            print(f\"\\n文本: {sent}\")\n",
    "            print(\"識別實體:\")\n",
    "            for ent in ents:\n",
    "                print(f\"- {ent['entity']} ({ent['type']})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"執行出錯: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
